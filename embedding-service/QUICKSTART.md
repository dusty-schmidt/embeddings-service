# QUICKSTART.md

# Get Running in 5 Minutes

## What You're Building

A **24/7 always-running service** that:
- ‚úÖ Generates embeddings for your chatbots (via API calls)
- ‚úÖ Automatically embeds files you drop in a folder
- ‚úÖ Stores everything in a searchable vector database
- ‚úÖ Never needs manual intervention after setup

## Prerequisites

- Docker and Docker Compose installed
- 4GB RAM minimum
- 10GB disk space

## Step-by-Step Setup

### 1. Get the Code (30 seconds)

```bash
# If you have the code already, skip to step 2
cd embedding-service

# Verify structure
ls -la app/ docker/ file_watcher/
```

### 2. Configure (1 minute)

```bash
# Copy environment template
cp .env.example .env

# Edit with your API keys
nano .env
```

**Minimum required changes:**
```bash
# Line 4-5: Change these to secure random strings!
API_KEYS=your-secret-key-here-change-me
ADMIN_KEYS=your-admin-key-here-change-me

# Line 28 (Optional): Add HuggingFace key for cloud fallback
HUGGINGFACE_API_KEY=hf_your_key_here
```

**Generate secure keys:**
```bash
python3 -c "import secrets; print(secrets.token_urlsafe(32))"
```

### 3. Start Everything (2 minutes)

```bash
# Navigate to docker directory
cd docker

# Start all services
docker-compose up -d

# Wait 30 seconds for containers to start
sleep 30

# Pull Ollama model (REQUIRED - first time only)
docker exec embedding-ollama ollama pull nomic-embed-text

# Verify it's running
curl http://localhost:8000/health
```

**Expected output:**
```json
{
  "status": "healthy",
  "service": "embedding-service",
  "cache_available": true,
  "providers": {
    "ollama": true
  }
}
```

### 4. Test It Works (1 minute)

```bash
# Get your API key from .env
API_KEY=$(grep "^API_KEYS=" ../.env | cut -d'=' -f2 | cut -d',' -f1)

# Test embedding
curl -X POST http://localhost:8000/api/embed \
  -H "X-API-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"text": "Hello world"}'

# Should return JSON with "embedding" field
```

**That's it! You're done!** üéâ

---

## Quick Usage Guide

### Drop Files to Auto-Embed

```bash
# Go back to project root
cd ..

# Create a test file
echo "Machine learning is a subset of artificial intelligence focused on learning from data." > knowledge/ml_basics.txt

# Watch it get processed (Ctrl+C to stop watching)
docker logs -f file-watcher
```

**You'll see:**
```
üìÑ Processing: /watch/ml_basics.txt
‚úÇÔ∏è  Split into 1 chunks
üîÑ Embedding chunk 1/1...
‚úì Stored chunk 1
‚úÖ Completed: /watch/ml_basics.txt
```

### Search Your Knowledge

```bash
# Search for relevant info
curl -X POST http://localhost:8000/knowledge/search \
  -H "X-API-Key: $API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is machine learning?", "limit": 3}'
```

### Use in Your Chatbot

```python
import requests

# Embed text
response = requests.post(
    "http://localhost:8000/api/embed",
    headers={"X-API-Key": "your-key-here"},
    json={"text": "your text"}
)
embedding = response.json()["embedding"]

# Search knowledge base
response = requests.post(
    "http://localhost:8000/knowledge/search",
    headers={"X-API-Key": "your-key-here"},
    json={"query": "your question", "limit": 5}
)
results = response.json()["results"]
```

---

## What's Running?

Check status:
```bash
docker ps

# Should show 5 containers:
# - embedding-service (API)
# - file-watcher
# - embedding-redis
# - embedding-ollama
# - embedding-qdrant
```

View logs:
```bash
docker logs -f embedding-service  # API logs
docker logs -f file-watcher       # File processing
docker logs -f embedding-ollama   # Ollama
```

Check metrics:
```bash
curl http://localhost:8000/metrics
```

---

## Common Commands

```bash
# Stop everything
cd docker && docker-compose down

# Restart everything
cd docker && docker-compose restart

# View all logs
cd docker && docker-compose logs -f

# Check disk usage
docker system df

# Backup your data
docker cp embedding-qdrant:/qdrant/storage ./backup-qdrant
tar -czf knowledge-backup.tar.gz knowledge/
```

---

## Next Steps

1. ‚úÖ **Add your files**: Drop .txt or .md files into `knowledge/`
2. ‚úÖ **Integrate your chatbot**: Use the API endpoints (see examples above)
3. ‚úÖ **Monitor**: Check `/metrics` endpoint regularly
4. ‚úÖ **Read full README**: See README.md for advanced features

---

## Troubleshooting

### "Ollama model not found"
```bash
docker exec embedding-ollama ollama pull nomic-embed-text
```

### "Connection refused"
```bash
# Check containers are running
docker ps

# If not, start them
cd docker && docker-compose up -d
```

### "File not processing"
```bash
# Check file watcher logs
docker logs file-watcher

# Verify file is .txt or .md
ls -la knowledge/

# Try touching the file
touch knowledge/yourfile.txt
```

### "Out of memory"
```bash
# Check memory usage
docker stats

# Increase Docker memory limit in Docker settings
# Or reduce number of services if needed
```

---

## Key Endpoints

```bash
# Health check
GET  http://localhost:8000/health

# Metrics
GET  http://localhost:8000/metrics

# Single embedding
POST http://localhost:8000/api/embed
Body: {"text": "..."}

# Batch embeddings
POST http://localhost:8000/api/embed/batch
Body: {"texts": ["...", "...", "..."]}

# Search knowledge
POST http://localhost:8000/knowledge/search
Body: {"query": "...", "limit": 5}

# Knowledge stats
GET  http://localhost:8000/knowledge/stats

# API docs (if DEBUG=true)
GET  http://localhost:8000/docs
```

---

## What Happens on Reboot?

**Everything restarts automatically!**

Docker Compose uses `restart: unless-stopped`, so:
1. System reboots
2. Docker daemon starts
3. All containers restart automatically
4. Service resumes exactly where it left off
5. Your data persists in Docker volumes

**No manual intervention needed!**

---

## Security Notes

üîí **Before production:**
- Change default API keys in `.env`
- Use HTTPS (add nginx reverse proxy)
- Enable firewall rules
- Regular backups

---

## Getting Help

**Check logs first:**
```bash
docker logs embedding-service
docker logs file-watcher
```

**Verify health:**
```bash
curl http://localhost:8000/health
curl http://localhost:8000/metrics
```

**Full documentation:**
- See `README.md` for complete guide
- See `docker/docker-compose.yml` for service config
- See API docs at http://localhost:8000/docs (if DEBUG=true)

---

## Summary

You now have:
- ‚úÖ 24/7 embedding API for your chatbots
- ‚úÖ Auto-ingestion of files dropped in `knowledge/`
- ‚úÖ Vector search for RAG applications
- ‚úÖ Redis caching for 80%+ cost savings
- ‚úÖ Local (Ollama) + Cloud (HuggingFace) options

**It runs forever. Just:**
1. Drop files when you want to add knowledge
2. Make API calls from your chatbots
3. Occasionally check metrics

**That's it!** üöÄ

---

**Need more details?** Read the full `README.md`

**Having issues?** Check the Troubleshooting section above

**Ready for production?** See Security Notes section
```

Both files are now complete and ready to use! üéâ